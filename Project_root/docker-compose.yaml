# x-airflow-common: &airflow-common
#   build:
#     context: .
#     dockerfile: docker/Dockerfile.airflow
#   image: ${AIRFLOW_IMAGE_NAME:-data_sentinel}
#   environment: &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#     AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
#     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
#     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
#     PYTHONPATH: /opt/airflow:/opt/airflow/src:/opt/airflow/plugins

#     DATA_STORAGE_DB_HOST: postgres_contain
#     DATA_STORAGE_DB_PORT: 5432
#     DATA_STORAGE_DB_USER: datauser
#     DATA_STORAGE_DB_PASSWORD: datapass
#     DATA_STORAGE_DB_NAME: data_sentinel
#   volumes:
#     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
#     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
#     - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
#     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
#     - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src
#     - ${AIRFLOW_PROJ_DIR:-.}/data:/data
#     - ${AIRFLOW_PROJ_DIR:-.}/sql:/opt/project_root/sql   # ✅ mount your sql folder
#   user: "${AIRFLOW_UID:-50000}:0"

# services:
#   airflow-webserver:
#     <<: *airflow-common
#     command: webserver
#     ports:
#       - "8080:8080"
#     depends_on:
#       - postgres
#       - redis
#       - postgres_contain
#     restart: always
#     networks:
#       - airflow-network

#   airflow-scheduler:
#     <<: *airflow-common
#     command: scheduler
#     depends_on:
#       - postgres
#       - redis
#       - postgres_contain
#     restart: always
#     networks:
#       - airflow-network

#   airflow-worker:
#     <<: *airflow-common
#     command: celery worker
#     depends_on:
#       - postgres
#       - redis
#       - postgres_contain
#     restart: always
#     networks:
#       - airflow-network

#   airflow-triggerer:
#     <<: *airflow-common
#     command: triggerer
#     depends_on:
#       - postgres
#       - redis
#       - postgres_contain
#     restart: always
#     networks:
#       - airflow-network







# # x-airflow-common: &airflow-common
# #   build:
# #     context: .
# #     dockerfile: docker/Dockerfile.airflow
# #   image: ${AIRFLOW_IMAGE_NAME:-data_sentinel}
# #   environment: &airflow-common-env
# #     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
# #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
# #     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
# #     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
# #     AIRFLOW__CORE__FERNET_KEY: ''
# #     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
# #     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
# #     AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
# #     AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
# #     _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
# #     PYTHONPATH: /opt/airflow:/opt/airflow/src:/opt/airflow/plugins

# #     # External Data Storage DB connection
# #     DATA_STORAGE_DB_HOST: postgres_contain
# #     DATA_STORAGE_DB_PORT: 5432
# #     DATA_STORAGE_DB_USER: datauser
# #     DATA_STORAGE_DB_PASSWORD: datapass
# #     DATA_STORAGE_DB_NAME: data_sentinel

# #   volumes:
# #     - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
# #     - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
# #     - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
# #     - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
# #     - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src
# #     - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
# #     - ${AIRFLOW_PROJ_DIR:-.}/data/staging:/opt/airflow/data/staging
# #     - ${AIRFLOW_PROJ_DIR:-.}/data/archive:/opt/airflow/data/archive
# #   user: "${AIRFLOW_UID:-50000}:0"

# # services:
# #   postgres:
# #     image: postgres:13
# #     environment:
# #       POSTGRES_USER: airflow
# #       POSTGRES_PASSWORD: airflow
# #       POSTGRES_DB: airflow
# #     volumes:
# #       - airflow-postgres-db-volume:/var/lib/postgresql/data
# #     restart: always
# #     networks:
# #       - airflow-network

# #   postgres_contain:
# #     image: postgres:13
# #     container_name: postgres_contain
# #     environment:
# #       POSTGRES_USER: datauser
# #       POSTGRES_PASSWORD: datapass
# #       POSTGRES_DB: data_sentinel
# #     volumes:
# #       - data-sentinel-db:/var/lib/postgresql/data
# #     ports:
# #       - "5433:5432"
# #     restart: always
# #     networks:
# #       - airflow-network

# #   redis:
# #     image: redis:latest
# #     restart: always
# #     networks:
# #       - airflow-network

# #   airflow-webserver:
# #     <<: *airflow-common
# #     command: webserver
# #     ports:
# #       - "8080:8080"
# #     depends_on:
# #       - postgres
# #       - redis
# #       - postgres_contain
# #     restart: always
# #     networks:
# #       - airflow-network

# #   airflow-scheduler:
# #     <<: *airflow-common
# #     command: scheduler
# #     depends_on:
# #       - postgres
# #       - redis
# #       - postgres_contain
# #     restart: always
# #     networks:
# #       - airflow-network

# #   airflow-worker:
# #     <<: *airflow-common
# #     command: celery worker
# #     depends_on:
# #       - postgres
# #       - redis
# #       - postgres_contain
# #     restart: always
# #     networks:
# #       - airflow-network

# #   airflow-triggerer:
# #     <<: *airflow-common
# #     command: triggerer
# #     depends_on:
# #       - postgres
# #       - redis
# #       - postgres_contain
# #     restart: always
# #     networks:
# #       - airflow-network

# #   airflow-init:
# #     <<: *airflow-common
# #     entrypoint: /bin/bash
# #     command:
# #       - -c
# #       - |
# #         mkdir -p /sources/logs /sources/dags /sources/plugins /sources/src /sources/data
# #         mkdir -p /sources/data/staging /sources/data/archive
# #         chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins,src,data}
# #         exec /entrypoint airflow version
# #     environment:
# #       <<: *airflow-common-env
# #       _AIRFLOW_DB_MIGRATE: 'true'
# #       _AIRFLOW_WWW_USER_CREATE: 'true'
# #       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-admin}
# #       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-admin}
# #       _PIP_ADDITIONAL_REQUIREMENTS: ''
# #     user: "0:0"
# #     volumes:
# #       - ${AIRFLOW_PROJ_DIR:-.}:/sources
# #     networks:
# #       - airflow-network

# #   streamlit:
# #     build:
# #       context: .
# #       dockerfile: docker/Dockerfile.streamlit
# #     container_name: streamlit
# #     ports:
# #       - "8501:8501"
# #     volumes:
# #       - ${AIRFLOW_PROJ_DIR:-.}/webapp:/app/webapp
# #       - ${AIRFLOW_PROJ_DIR:-.}/src:/app/src
# #       - ${AIRFLOW_PROJ_DIR:-.}/plugins:/app/plugins
# #       - ${AIRFLOW_PROJ_DIR:-.}/data:/app/data
# #     environment:
# #       PYTHONUNBUFFERED: 1
# #       PYTHONPATH: /app:/app/src:/app/plugins
# #       DB_HOST: postgres_contain
# #       DB_PORT: 5432
# #       DB_USER: datauser
# #       DB_PASSWORD: datapass
# #       DB_NAME: data_sentinel
# #       AIRFLOW_API_URL: http://airflow-webserver:8080/api/v1
# #       AIRFLOW_USERNAME: apiuser
# #       AIRFLOW_PASSWORD: apipass
# #       APP_TITLE: "Lupa - AI Data Sentinel"
# #       APP_MODE: dev
# #     depends_on:
# #       - postgres_contain
# #       - airflow-webserver
# #     restart: always
# #     networks:
# #       - airflow-network
# #     healthcheck:
# #       test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
# #       interval: 30s
# #       timeout: 10s
# #       retries: 3
# #       start_period: 40s

# #   minio:
# #     image: minio/minio:latest
# #     container_name: minio
# #     ports:
# #       - "9000:9000"
# #       - "9001:9001"
# #     volumes:
# #       - minio-data:/data
# #     environment:
# #       MINIO_ROOT_USER: minioadmin
# #       MINIO_ROOT_PASSWORD: minioadmin123
# #     command: server /data --console-address ":9001"
# #     restart: always
# #     networks:
# #       - airflow-network
# #     profiles:
# #       - storage

# # volumes:
# #   airflow-postgres-db-volume:
# #   data-sentinel-db:
# #   minio-data:

# # networks:
# #   airflow-network:
# #     driver: bridge

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: docker/Dockerfile.airflow
  image: ${AIRFLOW_IMAGE_NAME:-data_sentinel}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    PYTHONPATH: /opt/airflow:/opt/airflow/src:/opt/airflow/plugins

    # External DB for data storage
    DATA_STORAGE_DB_HOST: postgres_contain
    DATA_STORAGE_DB_PORT: 5432
    DATA_STORAGE_DB_USER: datauser
    DATA_STORAGE_DB_PASSWORD: datapass
    DATA_STORAGE_DB_NAME: data_sentinel
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src
    - ${AIRFLOW_PROJ_DIR:-.}/data:/data
    - ${AIRFLOW_PROJ_DIR:-.}/sql:/opt/project_root/sql   # ✅ schema.sql mounted here
  user: "${AIRFLOW_UID:-50000}:0"

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-postgres-db-volume:/var/lib/postgresql/data
    restart: always
    networks:
      - airflow-network

  postgres_contain:
    image: postgres:13
    container_name: postgres_contain
    environment:
      POSTGRES_USER: datauser
      POSTGRES_PASSWORD: datapass
      POSTGRES_DB: data_sentinel
    volumes:
      - data-sentinel-db:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    restart: always
    networks:
      - airflow-network

  redis:
    image: redis:latest
    restart: always
    networks:
      - airflow-network

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - redis
      - postgres_contain
    restart: always
    networks:
      - airflow-network

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      - postgres
      - redis
      - postgres_contain
    restart: always
    networks:
      - airflow-network

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    depends_on:
      - postgres
      - redis
      - postgres_contain
    restart: always
    networks:
      - airflow-network

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    depends_on:
      - postgres
      - redis
      - postgres_contain
    restart: always
    networks:
      - airflow-network

volumes:
  airflow-postgres-db-volume:
  data-sentinel-db:
  minio-data:

networks:
  airflow-network:
    driver: bridge
